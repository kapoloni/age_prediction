{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(62, 13)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "batches_lr_update = 5\n",
    "epochs = 2\n",
    "samples = 4000\n",
    "bs = 128\n",
    "n_iterations = epochs*samples//bs\n",
    "n_iterations, n_iterations//batches_lr_update+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = np.geomspace(1e-4, 1, n_iterations//batches_lr_update+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.00021544346900318845\n0.00021544346900318845\n0.00021544346900318845\n0.00021544346900318845\n0.00021544346900318845\n0.00046415888336127773\n0.00046415888336127773\n0.00046415888336127773\n0.00046415888336127773\n0.00046415888336127773\n0.001\n0.001\n0.001\n0.001\n0.001\n0.002154434690031882\n0.002154434690031882\n0.002154434690031882\n0.002154434690031882\n0.002154434690031882\n0.004641588833612777\n0.004641588833612777\n0.004641588833612777\n0.004641588833612777\n0.004641588833612777\n0.01\n0.01\n0.01\n0.01\n0.01\n0.021544346900318822\n0.021544346900318822\n0.021544346900318822\n0.021544346900318822\n0.021544346900318822\n0.046415888336127774\n0.046415888336127774\n0.046415888336127774\n0.046415888336127774\n0.046415888336127774\n0.1\n0.1\n0.1\n0.1\n0.1\n0.21544346900318823\n0.21544346900318823\n0.21544346900318823\n0.21544346900318823\n0.21544346900318823\n0.46415888336127775\n0.46415888336127775\n0.46415888336127775\n0.46415888336127775\n0.46415888336127775\n1.0\n1.0\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    print(learning_rates[iteration//batches_lr_update])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.00000000e-04, 2.15443469e-04, 4.64158883e-04, 1.00000000e-03,\n",
       "       2.15443469e-03, 4.64158883e-03, 1.00000000e-02, 2.15443469e-02,\n",
       "       4.64158883e-02, 1.00000000e-01, 2.15443469e-01, 4.64158883e-01,\n",
       "       1.00000000e+00])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "num_samples = 4000\n",
    "steps_per_epoch = np.ceil(num_samples / 128)\n",
    "sample_size=2048\n",
    "epochs = int(np.ceil(sample_size / float(steps_per_epoch)))\n",
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(256, 2048, 8.0, 4000)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "epochs, sample_size, steps_per_epoch, num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ceil((len(trainX) / float(config.BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import date\n",
    "\n",
    "from age_prediction.models.\\\n",
    "    efficientnet_pytorch_3d import EfficientNet3D as EfNetB0\n",
    "from age_prediction.data_module import MyDataModule\n",
    "from age_prediction.module_trainer import ModuleTrainer\n",
    "from age_prediction.callbacks import ModelCheckpoint, CyclicLR, \\\n",
    "                                      TensorBoardCB, CSVLogger\n",
    "from age_prediction.metrics import MSE, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "args = {}\n",
    "args['database'] = '../datasets'\n",
    "args['csv_data'] = '../csv_data'\n",
    "args['side'] = 'L'\n",
    "args['age_range'] = '[0,70]'\n",
    "args['gpu'] = 'True'\n",
    "args['gpu'] = 'False'\n",
    "args['dataParallel'] = 'True'\n",
    "args['data_aug'] = 'True'\n",
    "args['data_aug'] = 'False'\n",
    "args['snapshot'] = None\n",
    "args['batch_size'] = 512\n",
    "args['loss'] = 'MSE'\n",
    "args['optimizer'] = 'RMS'\n",
    "args['num_epochs'] = 2\n",
    "args['cyclicalLR'] = True\n",
    "args['weight_decay'] = '1e-5'\n",
    "args['clr'] = '[-3.1, -1.25]'\n",
    "args['dropout_rate'] = 0.2\n",
    "args = pd.DataFrame.from_dict(args, orient='index')\n",
    "args = args[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "database          ../datasets\n",
      "csv_data          ../csv_data\n",
      "side                        L\n",
      "age_range              [0,70]\n",
      "gpu                     False\n",
      "dataParallel             True\n",
      "data_aug                False\n",
      "snapshot                 None\n",
      "batch_size                512\n",
      "loss                      MSE\n",
      "optimizer                 RMS\n",
      "num_epochs                  2\n",
      "cyclicalLR               True\n",
      "weight_decay             1e-5\n",
      "clr             [-3.1, -1.25]\n",
      "dropout_rate              0.2\n",
      "Name: 0, dtype: object\n",
      "Preparing data\n",
      "Setup data\n",
      "Training with 700 evaluating with 74\n"
     ]
    }
   ],
   "source": [
    "print(args)\n",
    "\n",
    "# Load images\n",
    "if args.side == 'L':\n",
    "    side = \"_L\"\n",
    "else:\n",
    "    side = '_R'\n",
    "\n",
    "if args.loss == 'MSE':\n",
    "    loss = nn.MSELoss()\n",
    "    metrics = [MAE()]\n",
    "else:\n",
    "    loss = nn.L1Loss(reduction='mean')\n",
    "    metrics = [MSE()]\n",
    "\n",
    "if args.age_range is not None:\n",
    "    age_range = [int(args.age_range.split(\",\")[0].split(\"[\")[-1]),\n",
    "                int(args.age_range.split(\",\")[-1].split(\"]\")[0])]\n",
    "    if (age_range[0] == 0) & (age_range[1] == 70):\n",
    "        train_file = 'train_0-70.csv'\n",
    "        val_file = 'val_0-70.csv'\n",
    "    elif (age_range[0] == 70) & (age_range[1] == 100):\n",
    "        train_file = 'train_70-100.csv'\n",
    "        val_file = 'val_exp.csv'\n",
    "    else:\n",
    "        train_file = 'train_all.csv'\n",
    "        val_file = 'val_exp.csv'\n",
    "        print(\"Using a unknown age range\")\n",
    "\n",
    "\n",
    "dataloader = MyDataModule(database=args.database,\n",
    "                        csv_data=args.csv_data,\n",
    "                        side=side,\n",
    "                        batch=args.batch_size,\n",
    "                        data_aug=eval(args.data_aug),\n",
    "                        age_range=age_range,\n",
    "                        train_file=train_file,\n",
    "                        val_file=val_file\n",
    "                        )\n",
    "\n",
    "dataloader.prepare_data('fit')\n",
    "dataloader.setup('fit')\n",
    "\n",
    "train_size = len(dataloader.train.inputs[0])\n",
    "\n",
    "print(\"Training with\", train_size,\n",
    "    \"evaluating with\", len(dataloader.val.inputs[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dropout 0.2\nUsing cpu\n22-04-2021_age_[0-70]_RMS_wd_1e-5_L_dp0.2\nclr limits [-3.1, -1.25]\n"
     ]
    }
   ],
   "source": [
    "# Load effNet3D B0\n",
    "model = EfNetB0.from_name(\"efficientnet-b0\",\n",
    "                        override_params={\n",
    "                            'num_classes': 1,\n",
    "                            'dropout_rate': args.dropout_rate\n",
    "                        },\n",
    "                        in_channels=1,\n",
    "                        )\n",
    "if eval(args.gpu):\n",
    "    print(\"Using GPU\")\n",
    "    device = torch.device('cuda')\n",
    "    if eval(args.dataParallel):\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(torch.cuda.device_count(), \"GPUs!\")\n",
    "            model = nn.DataParallel(model)\n",
    "else:\n",
    "    print(\"Using cpu\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "if args.optimizer == 'RMS':\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(),\n",
    "                                    lr=.256, alpha=0.9,\n",
    "                                    eps=1e-08, momentum=0.9,\n",
    "                                    weight_decay=float(args.weight_decay))\n",
    "elif args.optimizer == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr=.01,\n",
    "                                weight_decay=float(args.weight_decay))\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=0.01, momentum=0.9,\n",
    "                                weight_decay=float(args.weight_decay))\n",
    "\n",
    "if args.snapshot is not None:\n",
    "    print('Loading model from {}'.format(args.snapshot))\n",
    "    checkpoint = torch.load(args.snapshot)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    _loss = checkpoint['loss']\n",
    "    _val_loss = checkpoint['val_loss']\n",
    "    print(\"Snapshot trained for {} epochs. \\\n",
    "        Loss: {} and Val loss {}\".format(epoch, _loss, _val_loss))\n",
    "\n",
    "output_folder = 'outputs'\n",
    "today = date.today()\n",
    "# dd/mm/YY\n",
    "d1 = today.strftime(\"%d-%m-%Y\")\n",
    "output_prefix = d1 + \"_age_\" + \\\n",
    "    \"-\".join(str(age_range).split(\" \")).replace(',', '') + \\\n",
    "    \"_\" + args.optimizer + \"_wd_\" + str(args.weight_decay) + \\\n",
    "    side + \"_dp\" + str(args.dropout_rate)\n",
    "\n",
    "print(output_prefix)\n",
    "callbacks = [CSVLogger(file=os.path.join(output_folder,\n",
    "                    'logger_' + output_prefix + '.csv'))]\n",
    "\n",
    "if args.cyclicalLR:\n",
    "    clr = [float(args.clr.split(\",\")[0].split(\"[\")[-1]),\n",
    "        float(args.clr.split(\",\")[-1].split(\"]\")[0])]\n",
    "    print('clr limits', clr)\n",
    "    # 10-3.1, 10-1.25\n",
    "    step_size = 6*(train_size//args.batch_size)\n",
    "    callbacks.append(CyclicLR(base_lr=10**clr[0],\n",
    "                            max_lr=10**clr[1],\n",
    "                            mode='triangular2',\n",
    "                            step_size=step_size\n",
    "                            )\n",
    "                    )\n",
    "\n",
    "callbacks.append(ModelCheckpoint(directory=output_folder,\n",
    "                                filename='ckpt_cpu_' +\n",
    "                                        output_prefix))\n",
    "\n",
    "callbacks.append(TensorBoardCB(log_dir='_'.join(\n",
    "                                        output_prefix.split(\"_\")[1:]\n",
    "                                        ),\n",
    "                                   max_img_grid=16,\n",
    "                                   imgs_batch=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1/2: 100%|██████████| 2/2 [00:45<00:00, 22.91s/ batches, loss=3001.4682, val_loss=2780.0710, mae=52.6280, val_mae=50.4824]\n",
      "Epoch 2/2: 100%|██████████| 2/2 [00:48<00:00, 24.18s/ batches, loss=7830802803834712.0000, val_loss=inf, mae=45565101.9832, val_mae=12225698487659921408.0000]It took 0 days 94 seconds and 189798 microseconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Train\n",
    "trainer = ModuleTrainer(model.to(device))\n",
    "\n",
    "trainer.compile(loss=loss,\n",
    "                optimizer=optimizer,\n",
    "                metrics=metrics,\n",
    "                callbacks=callbacks)\n",
    "\n",
    "trainer.fit_loader(dataloader.train_dataloader(),\n",
    "                dataloader.val_dataloader(),\n",
    "                num_epoch=int(args.num_epochs),\n",
    "                cuda_device=eval(args.gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "torch-dev",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}